{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnsemble method means applying different decision trees together.\\nIdea to apply different trees together is to improve the model and makes it better model.\\nThese decision trees are slow learners and together they creates model which is fast learner.\\n\\nWhat Ensemble Method Solves\\n===========================\\nIt solves the problem of overfitting.\\n\\nWhat is overfitting\\n===================\\nOverfitting means when testing accuracy is less than training accuracy.\\n\\nDifferent Ensemble Methods\\n==========================\\n1.) Random forest\\n2.) Bagging\\n3.) Boosting\\n4.) Stacking\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble method means applying different decision trees together.\n",
    "Idea to apply different trees together is to improve the model and makes it better model.\n",
    "These decision trees are slow learners and together they creates model which is fast learner.\n",
    "\n",
    "What Ensemble Method Solves\n",
    "===========================\n",
    "It solves the problem of overfitting.\n",
    "\n",
    "What is overfitting\n",
    "===================\n",
    "Overfitting means when testing accuracy is less than training accuracy.\n",
    "\n",
    "Different Ensemble Methods\n",
    "==========================\n",
    "1.) Random forest\n",
    "2.) Bagging\n",
    "3.) Boosting\n",
    "4.) Stacking\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow Random Forest works\\n=======================\\nTechnique Used: Bootstrap with replacement (Statistical Technique for randomnly selecting data points)\\nAlgorithm Used: Decision trees\\n\\nHow it works\\n============\\nFor each set of samples which is taken out with replacement that is those samples can be used again\\nFor each set of samples number of features taken out is sqrt(total features or variables)\\n\\nAdvantage of Random Forest\\n==========================\\nIt solves overfitting and missing value problem.\\n\\nCross Validation Techniques\\n===========================\\nDifferent cross validation techniques are used such as below\\n a.) Leave one out\\n b.) Leave p pout\\n c.) K-fold\\n d.) Repeated random sub sampling validation  \\n\\nWe are implementing K-fold for random forest\\n============================================\\n1.) During K fold we have value of k which denotes number of groups that a given data is to be split\\n2.) Accuracy or performance paramters is calculated 10 times as K is 10\\n3.) During each calculation one group is chosen as testing and rest others as training dataset, keeping in mind that\\ngroup once chosen for testing dataset is not repeated in the next step\\n\\nAverage of the accuracy during these 10 iterations is chosen as final accuracy\\n\\nSome Key points with respect to random forest method\\n=====================================================\\nFor classification: Majority of vote from decision tree is chosen as answer\\nFor Regression : Average of output from each decision tree is chosen as answer\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How Random Forest works\n",
    "=======================\n",
    "Technique Used: Bootstrap with replacement (Statistical Technique for randomnly selecting data points)\n",
    "Algorithm Used: Decision trees\n",
    "\n",
    "How it works\n",
    "============\n",
    "For each set of samples which is taken out with replacement that is those samples can be used again\n",
    "For each set of samples number of features taken out is sqrt(total features or variables)\n",
    "\n",
    "Advantage of Random Forest\n",
    "==========================\n",
    "It solves overfitting and missing value problem.\n",
    "\n",
    "Cross Validation Techniques\n",
    "===========================\n",
    "Different cross validation techniques are used such as below\n",
    " a.) Leave one out\n",
    " b.) Leave p pout\n",
    " c.) K-fold\n",
    " d.) Repeated random sub sampling validation  \n",
    "\n",
    "We are implementing K-fold for random forest\n",
    "============================================\n",
    "1.) During K fold we have value of k which denotes number of groups that a given data is to be split\n",
    "2.) Accuracy or performance paramters is calculated 10 times as K is 10\n",
    "3.) During each calculation one group is chosen as testing and rest others as training dataset, keeping in mind that\n",
    "group once chosen for testing dataset is not repeated in the next step\n",
    "\n",
    "Average of the accuracy during these 10 iterations is chosen as final accuracy\n",
    "\n",
    "Some Key points with respect to random forest method\n",
    "=====================================================\n",
    "For classification: Majority of vote from decision tree is chosen as answer\n",
    "For Regression : Average of output from each decision tree is chosen as answer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Random Forest on Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries to read the dataset\n",
    "import pandas as pd\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_read = pd.read_csv(\n",
    "           'pima-indians-diabetes.csv', \n",
    "            names = columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class_name\n",
       "0     6   148    72    35     0  33.6  0.627   50           1\n",
       "1     1    85    66    29     0  26.6  0.351   31           0\n",
       "2     8   183    64     0     0  23.3  0.672   32           1\n",
       "3     1    89    66    23    94  28.1  0.167   21           0\n",
       "4     0   137    40    35   168  43.1  2.288   33           1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_read.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dataset into data and target\n",
    "# from data I mean all the columns except last one\n",
    "# from the target I mean only the target column\n",
    "X = dataframe_read.iloc[:, 0:dataframe_read.shape[1]-1]\n",
    "y = dataframe_read.iloc[:, dataframe_read.shape[1]-1]\n",
    "\n",
    "# getting the features from the data\n",
    "features =  X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model = RandomForestClassifier(\n",
    "                    max_depth=2, \n",
    "                    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging classifier\n",
    "model_bagging = BaggingClassifier(\n",
    "                base_estimator=SVC(),\n",
    "                n_estimators=10, \n",
    "                random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada boosting\n",
    "model_adaboost = AdaBoostClassifier(\n",
    "      n_estimators=100, \n",
    "      random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For kfold we donot need to split the dataset into data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.71875\n",
      "Accuracy during 2 iterations is: 0.7421875\n",
      "Accuracy during 3 iterations is: 0.76953125\n",
      "Overall accuracy achieved using K fold is: 0.7434895833333334\n"
     ]
    }
   ],
   "source": [
    "# suppose we are working with three k fold\n",
    "kf3 = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.71875\n",
      "Accuracy during 2 iterations is: 0.7109375\n",
      "Accuracy during 3 iterations is: 0.73046875\n",
      "Overall accuracy achieved using K fold is: 0.7200520833333334\n"
     ]
    }
   ],
   "source": [
    "kf3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBagging Method: It is same as we have discussed Random forest except that all features are used \\nfor each sample set\\n\\nSalient features of Bagging Method\\n==================================\\n1.) Homogeneous Weak Learners are used to create model which is a better model\\n\\nFrom the homogeneous weak learner I mean all the algorithms or classifiers used are same\\nAll these models are run in parallel\\n\\nLet us implement baggin method on the above dataset with K fold\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bagging Method: It is same as we have discussed Random forest except that all features are used \n",
    "for each sample set\n",
    "\n",
    "Salient features of Bagging Method\n",
    "==================================\n",
    "1.) Homogeneous Weak Learners are used to create model which is a better model\n",
    "\n",
    "From the homogeneous weak learner I mean all the algorithms or classifiers used are same\n",
    "All these models are run in parallel\n",
    "\n",
    "Let us implement baggin method on the above dataset with K fold\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold without shuffle : Bagging Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.74609375\n",
      "Accuracy during 2 iterations is: 0.73828125\n",
      "Accuracy during 3 iterations is: 0.76953125\n",
      "Overall accuracy achieved using K fold is: 0.7513020833333334\n"
     ]
    }
   ],
   "source": [
    "kf3 = KFold(n_splits=3, shuffle=False, random_state=42)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        model_bagging.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold with Shuffle: Bagging Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.7578125\n",
      "Accuracy during 2 iterations is: 0.765625\n",
      "Accuracy during 3 iterations is: 0.73046875\n",
      "Overall accuracy achieved using K fold is: 0.7513020833333334\n"
     ]
    }
   ],
   "source": [
    "kf3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        model_bagging.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting\\n========\\n\\nThis is another ensemble Technique Here also homogeneous weak learners are used but the \\nprocess is sequential that is model runs sequentially. \\n\\nLet us understand Sequential Process\\n====================================\\n\\n1.) First first model is applied like decision tree or support vector classification \\n2.) Error is calculated and is propagated to the next model along with the sample set\\n\\nThis will take more time as models are not running in parallel\\n\\nExamples\\n=======\\n1.) Adaboost\\n2.) Gradient Boost\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Boosting\n",
    "========\n",
    "\n",
    "This is another ensemble Technique Here also homogeneous weak learners are used but the \n",
    "process is sequential that is model runs sequentially. \n",
    "\n",
    "Let us understand Sequential Process\n",
    "====================================\n",
    "\n",
    "1.) First first model is applied like decision tree or support vector classification \n",
    "2.) Error is calculated and is propagated to the next model along with the sample set\n",
    "\n",
    "This will take more time as models are not running in parallel\n",
    "\n",
    "Examples\n",
    "=======\n",
    "1.) Adaboost\n",
    "2.) Gradient Boost\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.7578125\n",
      "Accuracy during 2 iterations is: 0.765625\n",
      "Accuracy during 3 iterations is: 0.73046875\n",
      "Overall accuracy achieved using K fold is: 0.7513020833333334\n"
     ]
    }
   ],
   "source": [
    "kf3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        model_adaboost.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet us undestand Stacking\\n\\nStacking\\n========\\nModels are run in parallel and these models are heterogenous such as we can use svc,\\nrandom forest and naive bayes in parallel\\n\\nUse of different model can improve model accuracy to great extend\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us undestand Stacking\n",
    "\n",
    "Stacking\n",
    "========\n",
    "Models are run in parallel and these models are heterogenous such as we can use svc,\n",
    "random forest and naive bayes in parallel\n",
    "\n",
    "Use of different model can improve model accuracy to great extend\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of stacking\n",
    "# creating base estimators\n",
    "base_learners = [\n",
    "                 ('rf_1', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "                 ('rf_2', KNeighborsClassifier(n_neighbors=5))\n",
    "]             \n",
    "\n",
    "# creating the final estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators = base_learners, \n",
    "    final_estimator = LogisticRegression()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy during 1 iterations is: 0.7578125\n",
      "Accuracy during 2 iterations is: 0.765625\n",
      "Accuracy during 3 iterations is: 0.73046875\n",
      "Overall accuracy achieved using K fold is: 0.7513020833333334\n"
     ]
    }
   ],
   "source": [
    "## Implementing Stacking classifier\n",
    "kf3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "accuracy_total = list()\n",
    "for train_index, test_index in kf3.split(dataframe_read):\n",
    "        \n",
    "        X_train = dataframe_read.iloc[train_index].loc[:, features]\n",
    "        X_test = dataframe_read.iloc[test_index].loc[:, features]\n",
    "        y_train = dataframe_read.iloc[train_index].loc[:,'class_name']\n",
    "        y_test = dataframe_read.iloc[test_index].loc[:, 'class_name']\n",
    "        \n",
    "        stacking_model.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            y_test, model.predict(X_test)\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            \"Accuracy during {} iterations is: {}\".format(\n",
    "             i, accuracy)\n",
    "        )\n",
    "        i +=1\n",
    "        accuracy_total.append(accuracy) \n",
    "print(\n",
    "    \"Overall accuracy achieved using K fold is: {}\".format(\n",
    "        mean(accuracy_total))\n",
    ")                             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
